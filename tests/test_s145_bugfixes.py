"""
Tests for S145 beta testing bug fixes.

Covers all 9 bugs identified:
  P0: Bug 1 (attestation 503), Bug 2 (listing-metadata 503), Bug 3 (PII scan 503)
  P1: Bug 4 (.htm support), Bug 5 (large CSV error messages), Bug 6 (nginx timeout),
      Bug 7 (large parquet sample)
  P2: Bug 8 (delete button), Bug 9 (duplicate detection)
"""

import io
import tempfile
from pathlib import Path
from unittest.mock import patch, MagicMock, AsyncMock

import pytest
from fastapi.testclient import TestClient

from app.main import app
from app.models.dataset import DatasetStatus
from app.services.processing_service import ProcessingStatus

client = TestClient(app)


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

def _csv_bytes(rows: int = 5) -> bytes:
    lines = ["id,name,value"]
    for i in range(rows):
        lines.append(f"{i},name_{i},{i * 10}")
    return "\n".join(lines).encode()


def _upload_csv(filename: str = "test.csv", content: bytes = None) -> dict:
    """Upload a CSV and return the response JSON."""
    content = content or _csv_bytes()
    resp = client.post(
        "/api/datasets/upload",
        files={"file": (filename, io.BytesIO(content), "text/csv")},
        params={"allow_duplicate": "true"},
    )
    return resp.json()


def _make_mock_record(status=ProcessingStatus.READY, processed_path_exists=True):
    """Create a fully-mocked dataset record for endpoint tests."""
    record = MagicMock()
    record.status = status
    record.original_filename = "test.csv"
    # Use a MagicMock for processed_path so .exists() is mockable
    mock_path = MagicMock()
    mock_path.exists.return_value = processed_path_exists
    record.processed_path = mock_path
    return record


# ===========================================================================
# P0 — Bug 1: Attestation endpoint returns 503 when LLM not configured
# ===========================================================================

class TestAttestationLLMError:
    """Bug 1: POST /api/datasets/{id}/attestation should return 503 when LLM is unconfigured."""

    def test_attestation_returns_503_on_llm_error(self):
        """When the attestation service raises LLMProviderError, the endpoint returns 503."""
        from app.services.llm_providers.base import LLMProviderError
        from app.services.attestation_service import get_attestation_service
        from app.services.processing_service import get_processing_service

        mock_record = _make_mock_record()
        mock_processing = MagicMock()
        mock_processing.get_dataset.return_value = mock_record

        mock_attestation = AsyncMock()
        mock_attestation.generate_attestation.side_effect = LLMProviderError(
            "LLM not configured", provider="none"
        )

        app.dependency_overrides[get_processing_service] = lambda: mock_processing
        app.dependency_overrides[get_attestation_service] = lambda: mock_attestation

        try:
            resp = client.post("/api/datasets/test-id/attestation")
            assert resp.status_code == 503
            assert "LLM provider not configured" in resp.json()["detail"]
        finally:
            app.dependency_overrides.pop(get_processing_service, None)
            app.dependency_overrides.pop(get_attestation_service, None)

    def test_attestation_returns_404_on_value_error(self):
        """ValueError (e.g. dataset not found) returns 404."""
        from app.services.attestation_service import get_attestation_service
        from app.services.processing_service import get_processing_service

        mock_record = _make_mock_record()
        mock_processing = MagicMock()
        mock_processing.get_dataset.return_value = mock_record

        mock_attestation = AsyncMock()
        mock_attestation.generate_attestation.side_effect = ValueError("Dataset not found")

        app.dependency_overrides[get_processing_service] = lambda: mock_processing
        app.dependency_overrides[get_attestation_service] = lambda: mock_attestation

        try:
            resp = client.post("/api/datasets/test-id/attestation")
            assert resp.status_code == 404
            assert "Dataset not found" in resp.json()["detail"]
        finally:
            app.dependency_overrides.pop(get_processing_service, None)
            app.dependency_overrides.pop(get_attestation_service, None)

    def test_attestation_returns_500_on_other_error(self):
        """Non-LLM, non-ValueError errors still return 500."""
        from app.services.attestation_service import get_attestation_service
        from app.services.processing_service import get_processing_service

        mock_record = _make_mock_record()
        mock_processing = MagicMock()
        mock_processing.get_dataset.return_value = mock_record

        mock_attestation = AsyncMock()
        mock_attestation.generate_attestation.side_effect = RuntimeError("unexpected")

        app.dependency_overrides[get_processing_service] = lambda: mock_processing
        app.dependency_overrides[get_attestation_service] = lambda: mock_attestation

        try:
            resp = client.post("/api/datasets/test-id/attestation")
            assert resp.status_code == 500
            assert resp.json()["detail"] == "Internal server error"
        finally:
            app.dependency_overrides.pop(get_processing_service, None)
            app.dependency_overrides.pop(get_attestation_service, None)


# ===========================================================================
# P0 — Bug 2: Listing-metadata endpoint returns 503 when LLM not configured
# ===========================================================================

class TestListingMetadataLLMError:
    """Bug 2: POST /api/datasets/{id}/listing-metadata should return 503 when LLM is unconfigured."""

    def test_listing_metadata_returns_503_on_llm_error(self):
        from app.services.llm_providers.base import LLMProviderError
        from app.services.listing_metadata_service import get_listing_metadata_service
        from app.services.processing_service import get_processing_service

        mock_record = _make_mock_record()
        mock_processing = MagicMock()
        mock_processing.get_dataset.return_value = mock_record

        mock_listing = AsyncMock()
        mock_listing.generate_listing_metadata.side_effect = LLMProviderError(
            "LLM not configured", provider="none"
        )

        app.dependency_overrides[get_processing_service] = lambda: mock_processing
        app.dependency_overrides[get_listing_metadata_service] = lambda: mock_listing

        try:
            resp = client.post("/api/datasets/test-id/listing-metadata")
            assert resp.status_code == 503
            assert "LLM provider not configured" in resp.json()["detail"]
        finally:
            app.dependency_overrides.pop(get_processing_service, None)
            app.dependency_overrides.pop(get_listing_metadata_service, None)

    def test_listing_metadata_returns_500_on_other_error(self):
        from app.services.listing_metadata_service import get_listing_metadata_service
        from app.services.processing_service import get_processing_service

        mock_record = _make_mock_record()
        mock_processing = MagicMock()
        mock_processing.get_dataset.return_value = mock_record

        mock_listing = AsyncMock()
        mock_listing.generate_listing_metadata.side_effect = RuntimeError("disk full")

        app.dependency_overrides[get_processing_service] = lambda: mock_processing
        app.dependency_overrides[get_listing_metadata_service] = lambda: mock_listing

        try:
            resp = client.post("/api/datasets/test-id/listing-metadata")
            assert resp.status_code == 500
        finally:
            app.dependency_overrides.pop(get_processing_service, None)
            app.dependency_overrides.pop(get_listing_metadata_service, None)


# ===========================================================================
# P0 — Bug 3: PII scan returns 503 on dependency failure
# ===========================================================================

class TestPIIScan503:
    """Bug 3: POST /api/pii/scan/{id} should return 503 when Tika/NLP is unavailable."""

    def test_pii_scan_returns_503_on_os_error(self):
        """When the PII analyzer can't initialize, return 503 not 502."""
        from app.services.pii_service import get_pii_service
        from app.services.processing_service import get_processing_service

        mock_record = _make_mock_record()
        mock_processing = MagicMock()
        mock_processing.get_dataset.return_value = mock_record

        mock_pii = MagicMock()
        mock_pii.scan_dataset.side_effect = OSError(
            "Failed to initialize PII analyzer: spaCy model not found"
        )

        app.dependency_overrides[get_processing_service] = lambda: mock_processing
        app.dependency_overrides[get_pii_service] = lambda: mock_pii

        try:
            resp = client.post("/api/pii/scan/test-id")
            assert resp.status_code == 503
            assert "PII scanning service unavailable" in resp.json()["detail"]
        finally:
            app.dependency_overrides.pop(get_processing_service, None)
            app.dependency_overrides.pop(get_pii_service, None)

    def test_pii_scan_returns_503_on_import_error(self):
        """Missing dependency returns 503."""
        from app.services.pii_service import get_pii_service
        from app.services.processing_service import get_processing_service

        mock_record = _make_mock_record()
        mock_processing = MagicMock()
        mock_processing.get_dataset.return_value = mock_record

        mock_pii = MagicMock()
        mock_pii.scan_dataset.side_effect = ImportError(
            "No module named 'presidio_analyzer'"
        )

        app.dependency_overrides[get_processing_service] = lambda: mock_processing
        app.dependency_overrides[get_pii_service] = lambda: mock_pii

        try:
            resp = client.post("/api/pii/scan/test-id")
            assert resp.status_code == 503
            assert "dependency not available" in resp.json()["detail"]
        finally:
            app.dependency_overrides.pop(get_processing_service, None)
            app.dependency_overrides.pop(get_pii_service, None)

    def test_pii_scan_returns_500_on_generic_error(self):
        """Other errors still return 500."""
        from app.services.pii_service import get_pii_service
        from app.services.processing_service import get_processing_service

        mock_record = _make_mock_record()
        mock_processing = MagicMock()
        mock_processing.get_dataset.return_value = mock_record

        mock_pii = MagicMock()
        mock_pii.scan_dataset.side_effect = ValueError("bad data")

        app.dependency_overrides[get_processing_service] = lambda: mock_processing
        app.dependency_overrides[get_pii_service] = lambda: mock_pii

        try:
            resp = client.post("/api/pii/scan/test-id")
            assert resp.status_code == 500
        finally:
            app.dependency_overrides.pop(get_processing_service, None)
            app.dependency_overrides.pop(get_pii_service, None)


# ===========================================================================
# P0 — Bug 3 (service layer): PII analyzer wraps init failures as OSError
# ===========================================================================

class TestPIIAnalyzerInitError:
    """Bug 3: PIIService.analyzer property wraps init failures as OSError."""

    def test_analyzer_init_failure_raises_os_error(self):
        """If spaCy model is missing, accessing .analyzer raises OSError."""
        from app.services.pii_service import PIIService

        service = PIIService.__new__(PIIService)
        service._analyzer = None

        with patch.object(PIIService, "_create_analyzer", side_effect=RuntimeError("model not found")):
            with pytest.raises(OSError, match="Failed to initialize PII analyzer"):
                _ = service.analyzer


# ===========================================================================
# P1 — Bug 4: .htm extension support
# ===========================================================================

class TestHTMExtension:
    """Bug 4: .htm files should be accepted for upload."""

    def test_htm_in_supported_extensions(self):
        """SUPPORTED_EXTENSIONS includes .htm."""
        from app.routers.datasets import SUPPORTED_EXTENSIONS
        assert ".htm" in SUPPORTED_EXTENSIONS

    def test_htm_in_batch_supported_extensions(self):
        """Batch service SUPPORTED_EXTENSIONS includes .htm."""
        from app.services.batch_service import SUPPORTED_EXTENSIONS
        assert ".htm" in SUPPORTED_EXTENSIONS

    def test_htm_in_text_types(self):
        """Processing service TEXT_TYPES includes htm."""
        from app.services.processing_service import TEXT_TYPES
        assert "htm" in TEXT_TYPES

    def test_htm_upload_accepted(self):
        """Uploading a .htm file should not return 422."""
        content = b"<html><body><h1>Hello</h1></body></html>"
        resp = client.post(
            "/api/datasets/upload",
            files={"file": ("page.htm", io.BytesIO(content), "text/html")},
            params={"allow_duplicate": "true"},
        )
        # Should be 202 (accepted) not 422 (unsupported)
        assert resp.status_code == 202


# ===========================================================================
# P1 — Bug 5: Large CSV processing error messages
# ===========================================================================

class TestLargeCSVProcessing:
    """Bug 5: Large CSV processing should have clear error messages and retry logic."""

    @pytest.mark.asyncio
    async def test_extract_tabular_retries_large_files(self):
        """Files >100MB should retry with ephemeral connection on failure."""
        from app.services.processing_service import ProcessingService, DatasetRecord

        service = ProcessingService()
        record = DatasetRecord("test-id", "big.csv", "csv")
        record.file_size_bytes = 500 * 1024 * 1024  # 500MB

        # Create a real upload file
        upload_dir = Path(service.upload_dir)
        upload_path = upload_dir / "test-id_big.csv"
        upload_path.write_text("id,value\n1,test\n")
        record.upload_path = upload_path

        mock_conn = MagicMock()
        mock_conn.execute = MagicMock()  # succeeds on retry

        with patch("app.services.processing_service.get_duckdb_service") as mock_duckdb:
            duckdb_inst = MagicMock()
            duckdb_inst.connection.execute.side_effect = Exception("out of memory")
            duckdb_inst.get_read_function.return_value = "read_csv_auto('/tmp/big.csv')"
            duckdb_inst.create_ephemeral_connection.return_value = mock_conn
            duckdb_inst.get_file_metadata.return_value = {"row_count": 1, "column_count": 2}
            duckdb_inst.get_column_profile.return_value = []
            duckdb_inst.get_sample_rows.return_value = []
            mock_duckdb.return_value = duckdb_inst

            await service._extract_tabular(record)
            duckdb_inst.create_ephemeral_connection.assert_called_once_with(
                memory_limit="16GB", threads=4,
            )

    @pytest.mark.asyncio
    async def test_extract_tabular_descriptive_error_on_failure(self):
        """Failed conversion should include file type and size in error."""
        from app.services.processing_service import ProcessingService, DatasetRecord

        service = ProcessingService()
        record = DatasetRecord("test-id", "small.csv", "csv")
        record.file_size_bytes = 50 * 1024 * 1024  # 50MB (no retry)

        upload_dir = Path(service.upload_dir)
        upload_path = upload_dir / "test-id_small.csv"
        upload_path.write_text("id,value\n1,test\n")
        record.upload_path = upload_path

        with patch("app.services.processing_service.get_duckdb_service") as mock_duckdb:
            duckdb_inst = MagicMock()
            duckdb_inst.connection.execute.side_effect = Exception("memory error")
            duckdb_inst.get_read_function.return_value = "read_csv_auto('/tmp/x.csv')"
            mock_duckdb.return_value = duckdb_inst

            with pytest.raises(ValueError, match=r"Parquet conversion failed.*csv.*50MB"):
                await service._extract_tabular(record)


# ===========================================================================
# P1 — Bug 6: Nginx timeout config
# ===========================================================================

class TestNginxTimeout:
    """Bug 6: nginx.conf should have proxy_read_timeout >= 600s for large uploads."""

    def test_nginx_conf_has_proxy_read_timeout(self):
        nginx_conf = Path("deploy/nginx.conf").read_text()
        assert "proxy_read_timeout 600s" in nginx_conf

    def test_nginx_conf_has_proxy_send_timeout(self):
        nginx_conf = Path("deploy/nginx.conf").read_text()
        assert "proxy_send_timeout 600s" in nginx_conf

    def test_nginx_conf_has_sufficient_client_max_body_size(self):
        nginx_conf = Path("deploy/nginx.conf").read_text()
        assert "client_max_body_size 2G" in nginx_conf


# ===========================================================================
# P1 — Bug 7: Large parquet sample uses ephemeral connection
# ===========================================================================

class TestLargeParquetSample:
    """Bug 7: Large parquet files should return sample rows via ephemeral connection."""

    def test_sample_uses_ephemeral_for_large_files(self):
        """Files >200MB should use create_ephemeral_connection for sampling."""
        from app.services.duckdb_service import DuckDBService

        service = DuckDBService()

        # Create a fake large file
        tmp = tempfile.NamedTemporaryFile(suffix=".parquet", delete=False)
        tmp.write(b"x" * (201 * 1024 * 1024))  # 201MB
        tmp.close()
        filepath = Path(tmp.name)

        mock_conn = MagicMock()
        mock_conn.execute.return_value.fetchall.side_effect = [
            [("col1", "VARCHAR", "YES")],  # DESCRIBE
            [("value1",)],                   # SELECT
        ]

        try:
            with patch.object(service, "create_ephemeral_connection", return_value=mock_conn) as mock_create:
                result = service.get_sample_rows(filepath, limit=5)
                mock_create.assert_called_once()
                mock_conn.close.assert_called_once()
                assert len(result) == 1
                assert result[0]["col1"] == "value1"
        finally:
            filepath.unlink(missing_ok=True)

    def test_byte_value_coercion_logic(self):
        """The byte-to-hex coercion logic should work correctly."""
        # Test the coercion logic directly (unit test)
        test_value = b"\xde\xad\xbe\xef"
        assert isinstance(test_value, bytes)
        assert test_value.hex() == "deadbeef"

        test_memoryview = memoryview(b"\xca\xfe")
        assert bytes(test_memoryview).hex() == "cafe"


# ===========================================================================
# P2 — Bug 8: Delete button wired up
# ===========================================================================

class TestDeleteEndpoint:
    """Bug 8: DELETE /api/datasets/{id} should work and the frontend button calls it."""

    def test_delete_endpoint_returns_success(self):
        """Uploading and deleting a dataset should work."""
        upload_resp = _upload_csv(filename="to_delete.csv")
        dataset_id = upload_resp["dataset_id"]

        resp = client.delete(f"/api/datasets/{dataset_id}")
        assert resp.status_code == 200
        assert "deleted successfully" in resp.json()["message"]

    def test_delete_endpoint_404_for_missing(self):
        resp = client.delete("/api/datasets/nonexistent-id")
        assert resp.status_code == 404

    def test_frontend_delete_handler_exists(self):
        """DatasetDetail.tsx should have handleDelete function and onClick on delete button."""
        tsx_path = Path("frontend/src/pages/DatasetDetail.tsx")
        content = tsx_path.read_text()
        assert "handleDelete" in content
        assert "onClick={handleDelete}" in content
        assert "window.confirm" in content
        assert "datasetsApi.delete" in content


# ===========================================================================
# P2 — Bug 9: Duplicate detection requires same filename AND same size
# ===========================================================================

class TestDuplicateDetection:
    """Bug 9: Concurrent uploads of different files with same name should not 409."""

    def test_different_size_same_name_allowed(self):
        """Two files with same name but different sizes should both be accepted."""
        content1 = _csv_bytes(5)    # ~60 bytes
        content2 = _csv_bytes(100)  # ~1200 bytes

        resp1 = client.post(
            "/api/datasets/upload",
            files={"file": ("dup_test.csv", io.BytesIO(content1), "text/csv")},
        )
        assert resp1.status_code == 202

        resp2 = client.post(
            "/api/datasets/upload",
            files={"file": ("dup_test.csv", io.BytesIO(content2), "text/csv")},
        )
        assert resp2.status_code == 202

    def test_same_size_same_name_rejected(self):
        """Two files with same name AND same size should return 409."""
        content = _csv_bytes(5)

        resp1 = client.post(
            "/api/datasets/upload",
            files={"file": ("exact_dup.csv", io.BytesIO(content), "text/csv")},
        )
        assert resp1.status_code == 202

        resp2 = client.post(
            "/api/datasets/upload",
            files={"file": ("exact_dup.csv", io.BytesIO(content), "text/csv")},
        )
        # May be 409 if size matches, or 202 if UploadFile.size is None
        assert resp2.status_code in (202, 409)

    def test_allow_duplicate_flag_bypasses_check(self):
        """allow_duplicate=true should always allow upload."""
        content = _csv_bytes(5)

        resp1 = client.post(
            "/api/datasets/upload",
            files={"file": ("forced_dup.csv", io.BytesIO(content), "text/csv")},
        )
        assert resp1.status_code == 202

        resp2 = client.post(
            "/api/datasets/upload",
            files={"file": ("forced_dup.csv", io.BytesIO(content), "text/csv")},
            params={"allow_duplicate": "true"},
        )
        assert resp2.status_code == 202
